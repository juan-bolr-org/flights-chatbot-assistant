{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt\n",
    "# carga de bibliotecas para el taller\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instancia del modelo\n",
    "Crearemos un llamado al modelo gpt-4.1\n",
    "¿Qué temperatura deberíamos ponerle a un RAG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_temperature = 0.0 # todo: Colocar la temperatura adecuada según lo aprendido\n",
    "response_model = init_chat_model(\"openai:gpt-4.1\", temperature=model_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a friendly and helpful virtual assistant specialized in providing flight information. Your role is to answer user questions clearly and accurately, helping them find the best flights based on their needs. Always respond with a warm, polite, and professional tone. Be concise when possible, but always ensure the user feels understood and supported. Never be rude, aggressive, or use inappropriate language. Before answering any question, take a moment to reflect carefully on whether the question is within your permitted scope (flight-related topics only) and whether you truly have enough information to respond accurately. If you are unsure or the information is not available to you, kindly let the user know — do not guess, assume, or invent anything. You do not have access to the internet, so you cannot look up information. You are strictly limited to answering questions related to flights. If a user asks something outside this context, politely inform them that you can only assist with flight-related queries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# System prompts for the chatbot\n",
    "system_prompts = [SYSTEM_PROMPT]\n",
    "\n",
    "# Separar el texto y codificar con el splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "flight_splits = text_splitter.create_documents(system_prompts)\n",
    "\n",
    "# flight_splits ahora contiene los chunks listos para el embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Crear la base de datos en memoria\n",
    "\n",
    "Al crear la base de datos, se necesita codificar los documentos con el modelo de Embedding. Ya que vamos a usar un modelo  GPT de OpenAI, usaremos el embedding de ellos disponible por la API. Usaremos un objeto retriever para hacer las búsquedas semánticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=flight_splits, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30})\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_flights\",\n",
    "    \"Retrive flight information based on user queries\",\n",
    ")\n",
    "\n",
    "# consultar la base de datos vectorial\n",
    "query_text = \"\" # todo: escribir aca el texto query a buscar en la base de datos\n",
    "retriever_tool.invoke({\"query\": query_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Consulta usando el RAG\n",
    "Ahora que tenemos una base de datos vectorial, consultaremos primero la pregunta, traemos un contexto y le pasamos todo el paquete al modelo de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar la respuesta del RAG\n",
    "def generate_rag_answer(state: MessagesState, use_database=True):\n",
    "    if use_database:\n",
    "        # utilizar la base de datos\n",
    "        docs = retriever_tool.invoke({\"query\": state[\"messages\"][-1][\"content\"]})\n",
    "        print(\"Text retrieved from db:\\n\", docs)\n",
    "        full_prompt = f'Context:\\n{docs}\\n\\nUser Query: { state[\"messages\"][-1][\"content\"] }'\n",
    "        response = response_model.invoke(full_prompt)\n",
    "    else:\n",
    "        # no utilizar la base de datos\n",
    "        response = response_model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probando con una entrada cualquiera\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"busca en internet ese dato, lo necesito de forma urgente!!\"\n",
    "        }        \n",
    "    ]\n",
    "}\n",
    "\n",
    "response = generate_rag_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probando con una pregunta\n",
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"cual es el vuelo con mayor departure time?\"\n",
    "        }        \n",
    "    ]\n",
    "}\n",
    "\n",
    "response = generate_rag_answer(input, True)[\"messages\"][-1].content  # ajustar si se quiere usar o no el RAG\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar solo los documentos\n",
    "import pickle\n",
    "with open(\"flight_docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(flight_splits, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
